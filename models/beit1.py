# -*- coding: utf-8 -*-
"""Beit1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y4E4ehmWncUaye4B3MZQuR09THIlLSKf
"""

from google.colab import drive
drive.mount('/content/drive')

zip_path = "/content/drive/MyDrive/yenidataset_split.zip"

dest_dir = "/content/yenidataset_split3"

import zipfile
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(dest_dir)

print(f"âœ… {zip_path} iÃ§eriÄŸi {dest_dir} klasÃ¶rÃ¼ne Ã§Ä±karÄ±ldÄ±.")

import os
import time
import random
import numpy as np
import torch
import torch.nn as nn
import timm

from torchvision import transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, roc_curve, auc
)
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

# â”€â”€â”€ 1ï¸âƒ£ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EPOCHS       = 30
BATCH_SIZE   = 32
IMG_SIZE     = 224
NUM_CLASSES  = 5
DATA_PATH    = "/content/yenidataset_split3/yenidataset_split"
DEVICE       = torch.device("cuda" if torch.cuda.is_available() else "cpu")
SEED         = 42
LR           = 1e-5
WEIGHT_DECAY = 2e-2

torch.manual_seed(SEED)
np.random.seed(SEED)
random.seed(SEED)
if DEVICE.type == 'cuda':
    torch.cuda.manual_seed(SEED)

# â”€â”€â”€ 2ï¸âƒ£ TRANSFORMS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]

train_transform = transforms.Compose([
    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(30),
    transforms.ColorJitter(0.3, 0.3, 0.3, 0.2),
    transforms.RandomPerspective(distortion_scale=0.4, p=0.5),
    transforms.RandomAffine(degrees=20, translate=(0.1, 0.1)),
    transforms.ToTensor(),
    transforms.Normalize(mean, std)
])

val_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean, std)
])

test_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean, std)
])

# â”€â”€â”€ 3ï¸âƒ£ DATASETS & DATALOADERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
train_ds = ImageFolder(os.path.join(DATA_PATH, "train"), transform=train_transform)
val_ds   = ImageFolder(os.path.join(DATA_PATH, "val"),   transform=val_transform)
test_ds  = ImageFolder(os.path.join(DATA_PATH, "test"),  transform=test_transform)

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,
                          num_workers=2, pin_memory=True)
val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,
                          num_workers=2, pin_memory=True)
test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,
                          num_workers=2, pin_memory=True)

# â”€â”€â”€ 4ï¸âƒ£ BEiT MODEL + LOSS + OPTIMIZER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model = timm.create_model(
    'beit_base_patch16_224',
    pretrained=True,
    num_classes=NUM_CLASSES,
    drop_rate=0.5,
    drop_path_rate=0.3
).to(DEVICE)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)

def mixup_data(x, y, alpha=0.4):
    lam = np.random.beta(alpha, alpha)
    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(DEVICE)
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

# â”€â”€â”€ 5ï¸âƒ£ TRAIN + VALIDATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
train_losses, val_losses = [], []

t0 = time.time()
for epoch in range(1, EPOCHS+1):
    t_epoch = time.time()

    # â€” Train â€”
    model.train()
    run_train = 0.0
    for imgs, lbls in tqdm(train_loader, desc=f"Train {epoch}/{EPOCHS}", leave=False):
        imgs, lbls = imgs.to(DEVICE), lbls.to(DEVICE)

        # âœ… Mixup kullanÄ±mÄ±
        inputs, targets_a, targets_b, lam = mixup_data(imgs, lbls, alpha=0.4)

        optimizer.zero_grad()
        out = model(inputs)
        loss = lam * criterion(out, targets_a) + (1 - lam) * criterion(out, targets_b)
        loss.backward()
        optimizer.step()
        run_train += loss.item()
    train_loss = run_train / len(train_loader)
    train_losses.append(train_loss)

    # â€” Validation â€”
    model.eval()
    run_val = 0.0
    with torch.no_grad():
        for imgs, lbls in val_loader:
            imgs, lbls = imgs.to(DEVICE), lbls.to(DEVICE)
            out = model(imgs)
            val_loss_batch = criterion(out, lbls)
            run_val += val_loss_batch.item()
    val_loss = run_val / len(val_loader)
    val_losses.append(val_loss)

    # âœ… Scheduler Update
    scheduler.step()

    print(f"Epoch {epoch}/{EPOCHS} â€” {(time.time()-t_epoch):.1f}s | Train: {train_loss:.4f} | Val: {val_loss:.4f}")

# Final model save after all epochs
torch.save(model.state_dict(), "final_beit_model.pth")
print(f"\nğŸ Training completed in {(time.time()-t0)/60:.1f}m")

# â”€â”€â”€ 6ï¸âƒ£ TEST â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model.load_state_dict(torch.load("final_beit_model.pth"))
model.eval()

all_labels, all_preds, all_probs = [], [], []
t_inf = time.time()
with torch.no_grad():
    for imgs, lbls in tqdm(test_loader, desc="Test Inference"):
        imgs = imgs.to(DEVICE)
        out  = model(imgs)
        probs = torch.softmax(out, dim=1).cpu().numpy()
        preds = np.argmax(probs, axis=1)
        all_probs.extend(probs)
        all_preds.extend(preds)
        all_labels.extend(lbls.numpy())
inf_time = time.time() - t_inf

all_labels = np.array(all_labels)
all_preds  = np.array(all_preds)
all_probs  = np.array(all_probs)

# â”€â”€â”€ 7ï¸âƒ£ METRICS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
conf = confusion_matrix(all_labels, all_preds)
acc   = accuracy_score(all_labels, all_preds)
prec  = precision_score(all_labels, all_preds, average='macro')
rec   = recall_score(all_labels, all_preds, average='macro')
f1    = f1_score(all_labels, all_preds, average='macro')
auc_m = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='micro')
auc_M = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='macro')

# Specificity
specs = []
for i in range(NUM_CLASSES):
    TP = conf[i,i]
    FN = conf[i,:].sum() - TP
    FP = conf[:,i].sum() - TP
    TN = conf.sum() - TP - FN - FP
    specs.append(TN / (TN + FP))
spec_macro = np.mean(specs)

print(f"\nFinal Test Metrics:")
print(f" â€¢ Accuracy       : {acc:.4f}")
print(f" â€¢ Precision (M)  : {prec:.4f}")
print(f" â€¢ Recall    (M)  : {rec:.4f}")
print(f" â€¢ F1-Score  (M)  : {f1:.4f}")
print(f" â€¢ AUC-micro      : {auc_m:.4f}")
print(f" â€¢ AUC-macro      : {auc_M:.4f}")
print(f" â€¢ Sensitivity (M): {rec:.4f}")
print(f" â€¢ Specificity (M): {spec_macro:.4f}")
print(f" â€¢ Train Time     : {(time.time()-t0):.1f}s")
print(f" â€¢ Inference Time : {inf_time:.1f}s")

# â”€â”€â”€ 8ï¸âƒ£ PLOTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
plt.figure(figsize=(7,6))
sns.heatmap(conf, annot=True, fmt='d', cmap='Blues',
            xticklabels=test_ds.classes, yticklabels=test_ds.classes)
plt.title("Confusion Matrix"); plt.xlabel("Predicted"); plt.ylabel("Actual"); plt.show()

plt.figure(figsize=(8,6))
fpr, tpr, roc_auc = {}, {}, {}
for i, cls in enumerate(test_ds.classes):
    fpr[i], tpr[i], _ = roc_curve((all_labels==i).astype(int), all_probs[:,i])
    roc_auc[i] = auc(fpr[i], tpr[i])
    plt.plot(fpr[i], tpr[i], label=f"{cls} (AUC={roc_auc[i]:.2f})")
plt.plot([0,1],[0,1],'k--'); plt.title("ROC Curves"); plt.xlabel("FPR"); plt.ylabel("TPR"); plt.legend(); plt.show()

plt.figure(figsize=(8,5))
plt.plot(range(1, len(train_losses)+1), train_losses, label='Train Loss')
plt.plot(range(1, len(val_losses)+1),   val_losses,   label='Val Loss')
plt.title("Epoch vs Loss"); plt.xlabel("Epoch"); plt.ylabel("Loss"); plt.legend(); plt.show()

# ğŸ”Ÿ Model Save
torch.save(model.state_dict(), "beit_model.pth")
print(f"\nâœ… BEiT Model saved: beit_model.pth")